---
title: "Human Activity Recognition"
author: "Jeff Shilling"
date: "Saturday, February 21, 2015"
output: html_document
---
```{r, echo=FALSE,message=FALSE}
library(caret)
library(randomForest)
```
##Description
This assignment attempts to build a model that uses data from wearable sensors to predict the performance quality of a subject curling a dumbbell.  The prediction uses the sensor data to determine if the subject performed the curl correctly or made one of four common mistakes.  The data come from here:  

>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.    

The approach used for this model follows:  
1. Download the training dataset, clean it up a bit, and partition 80% into a training set and 20% into a validation set  
2. Build a model using the caret package train function with the random forest method with the cross validation option turned on  
3. Assess the accuracy of the model and repeat training with using different variables until the accuracy is sufficient  
4. Validate the model on the validation dataset (the remaining 20% of the original training data)  

###Data Preparation
Load the data:
```{r, echo=TRUE}
file <- "pml-training.csv"
URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
originalData <- read.csv(file, header = T, fill = TRUE, stringsAsFactors=FALSE)
dim(originalData)
```

A quick check shows there are `r ncol(originalData)` variables.  I cleaned up the data in the following steps:  
1. Removed summary rows indicated by the new_window variable
2. Changed the response (classe) variable to a factor
3. Dropped the labeling variables as non-predictors
4. Removed variables with so many #DIV/0! values they were identified as characters
6. Removed variables with too many NA values for random forest to handle
7. Removed variables with near zero variance  
```{r,echo=TRUE}
working <- originalData
### remove summary rows and make the response variable a factor
working <- working[working$new_window=="no",]
working$classe <- as.factor(working$classe) 
### drop the non-predictive labelling variables
working <- working[,-c(1:6)]
### remove variables that won't work with random forest
### sepcifically, variable that are characters because of missing data
keep <- !(sapply(working, is.character))
working <- working[,keep]
missing <- sapply(working, function(x) sum(is.na(x)) > 0)
working <- working[,!missing]
### remove variables with near zero variance
NZV <- nearZeroVar(working[,-1], saveMetrics=TRUE)$nzv
working <- working[,!NZV]
dim(working)
```

The clean up reduce the number of potential predictors to `r ncol(working)-1`.  

I split the "working" dataset into a traing dataset and a "test" dataset that I call "validation" here to distinguish it from the official test data.
```{r, echo=TRUE}
set.seed(6789)
inTrain <- createDataPartition(y=working$classe , p=0.8, list=FALSE)
training <- working[inTrain,]
validation <- working[-inTrain,]
```


###Build a machine learning algorithm to predict activity quality from activity monitors
Because of the advice in week 3, lecture 3 that "[r]andom forests are usually one of the two top performing algorithms along with boosting in prediction contests," I chose to try that first.  
```{r, echo=TRUE}
fit <- randomForest(classe ~ ., data=training)
fit
```
That appears to be a pretty good model, but also one that could easily be overfit.

###The expected out of sample error to be and estimated error using cross-validation
In addition to the built in cross-validation that random forest provided, I tested the model against my reserved validation dataset.  
```{r, echo=TRUE}
predictions <- predict(fit, validation)
confusionMatrix(predictions, validation$classe)
```

Here is the plot of those predictions followed by the Out Of Sample results:  
```{r, fig.height=4,fig.width=4}
qplot(predictions,classe,data=validation)

outOfSample <- data.frame(Accuracy = sum(predictions == validation$classe)/length(predictions))
outOfSample$Error <- 1 - outOfSample$Accuracy
print(outOfSample, digits=3)
```

The estimated Out of Sample Error rate is `r round(outOfSample$Error*100, digits=3)`%.  

###Final Application
I created predictions using the model generated by random forest as follows (not executed):
```{r, echo=TRUE}
#  testPredictions <- as.character(predict(fit, testing))
```
All 20 of the generated preditions were correct.




